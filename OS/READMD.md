<details>
<summary style="font-size:20px">Transactional이란</summary>
<div markdown="1">
<br/>
<h3>🤔 Transactional은 무엇을 하나요?</h3>

- Transactional은 **하나의 논리적 작업 단위로 수행되는 일련의 작업**을 이야기합니다.
<br/>

<h3>🤔 Transactional 특성은 어떤 것이 있나요?</h3>

- Transactiona이 가져야 하는 특성에는 ACID이 있습니다.

✅ Atomicity: 원자성은 트랜잭션과 관련된 작업들이 부분적으로 실행되다가 중단되지 않는 것을 보장하는 능력이다. 예를 들어, 자금 이체는 성공할 수도 실패할 수도 있지만 보내는 쪽에서 돈을 빼 오는 작업만 성공하고 받는 쪽에 돈을 넣는 작업을 실패해서는 안된다. 원자성은 이와 같이 중간 단계까지 실행되고 실패하는 일이 없도록 하는 것이다.

✅ Consistency: 일관성은 트랜잭션이 실행을 성공적으로 완료하면 언제나 일관성 있는 데이터베이스 상태로 유지하는 것을 의미한다. 무결성 제약이 모든 계좌는 잔고가 있어야 한다면 이를 위반하는 트랜잭션은 중단된다.

✅ Isolation: 고립성은 트랜잭션을 수행 시 다른 트랜잭션의 연산 작업이 끼어들지 못하도록 보장하는 것을 의미한다. 이것은 트랜잭션 밖에 있는 어떤 연산도 중간 단계의 데이터를 볼 수 없음을 의미한다. 은행 관리자는 이체 작업을 하는 도중에 쿼리를 실행하더라도 특정 계좌간 이체하는 양 쪽을 볼 수 없다. 공식적으로 고립성은 트랜잭션 실행내역은 연속적이어야 함을 의미한다. 성능관련 이유로 인해 이 특성은 가장 유연성 있는 제약 조건이다. 자세한 내용은 관련 문서를 참조해야 한다.

✅ Durability: 지속성은 성공적으로 수행된 트랜잭션은 영원히 반영되어야 함을 의미한다. 시스템 문제, DB 일관성 체크 등을 하더라도 유지되어야 함을 의미한다. 전형적으로 모든 트랜잭션은 로그로 남고 시스템 장애 발생 전 상태로 되돌릴 수 있다. 트랜잭션은 로그에 모든 것이 저장된 후에만 commit 상태로 간주될 수 있다.

<h3>❓ Spring Project에서 @Transactional은 어떻게 동작하나요?</h3>

- @Transactional를 통해 DB와의 상호작용이 Transactional으로 묶일 수 있습니다.

1. 메서드가 호출되면 Spring은 Transactional을 시작합니다.
2. 메서드 내에서 수행되는 DB 작업은 Transactional 내에서 실행됩니다. 이러한 작업은 일련의 데이터 변경 작업이 포함될 수 있습니다.
3. 메서드가 성공적으로 완료되면, Spring은 Transaction을 Commit하여 DB의 변경 사항을 영구적으로 반영합니다.
4. 메서드 실행 도중 예외가 발생하면, Spring은 Transaction을 롤백하고 이전 상태로 DB를 되돌립니다. 이는 메서드 내에서 발생한 모든 변경 사항을 취소합니다.

<h3>❓ Transactional을 사용할 때, DataSource 이야기가 많이 나오는데 어떤 상관관계가 존재하나요?</h3>

- DataSource는 Spring에서 DB와의 연결을 관리하는 Interface입니다. Spring은 DataSource Interface를 사용하여 DB ConnectionPool을 구성하고, DB 연결을 관리하며, DB와의 상호작용을 처리합니다.

- DataSource는 DB 연결 설정 및 Transaction 관리 등을 제공합니다. @Transactional 어노테이션과 함께 사용될 때 데이터베이스 연결과 트랜잭션 관리를 담당하는 주요한 요소입니다.

</div>
</details>

<details>
<summary style="font-size:20px">시스템 콜은 무엇인가요?</summary>
<div markdown="1">
<br/>
<h2>🤔 시스템 콜이란?</h2>

<h3>📝 나의 대답</h3>

- 시스템 콜이란 프로세스가 시스템 자원을 사용하기 위해 운영체제에게 자원을 할당 받기 위해 사용되는 트리거입니다.

<h3>GPT의 대답</h3>

- 시스템 콜은 운영체제의 기능과 자원을 사용하기 위해 프로세스가 운영체제에게 요청하는 인터페이스 또는 메커니즘입니다.

<br/>
<h2>🤔 우리가 사용하는 시스템 콜의 예시를 들어주세요</h2>

<h3>📝 나의 대답</h3>

- 예를 들어 프로세스에서 연산이 필요한 필요한 작업을 수행할 때 프로세스는 CPU를 통해 연산을 수행해야하는데 이 때 연산이 필요한 프로세스는 운영체제에게 시스템 콜을 요청합니다.

<h3>GPT의 대답</h3>

- 시스템 콜은 다양한 작업을 포함하며, 예들 들어 파일 입출력, 네트워크 통신, 메모리 관리 등을 포함하고 있습니다.

<br/>
<h2>🤔 시스템 콜이 운영체제에서 어떤 과정으로 실행되는지 설명해 주세요</h2>

<h3>📝 나의 대답</h3>

- 프로세스가 원한다고 해서 즉시 운영체제에게 시스템 콜을 발생해 요청할 수 없습니다. 운영체제의 스케쥴링에 따라 자원을 할당 받을 수 있으며 준비 - 대기 - 실행 상태를 순회하며 컴퓨터 자원을 할당 받습니다.

<h3>GPT의 대답</h3>

- 시스템 콜이 운영체제에서 실행되는 과정은 다음과 같습니다:

1. 프로세스가 시스템 콜을 호출합니다. 이는 프로세스가 운영체제의 특정 기능 또는 자원에 접근하기 위해 필요한 요청입니다.
2. 운영체제는 시스템 콜을 감지하고, 해당 요청을 처리하기 위해 해당 기능 또는 자원에 대한 작업을 시작합니다.
3. 운영체제는 프로세스의 상태를 변경하여 시스템 콜을 처리합니다. 이는 대부분의 경우 프로세스를 사용 가능한 대기 상태로 변경합니다.
4. 운영체제는 필요한 작업을 수행하고, 요청된 자원 또는 기능을 프로세스에 할당합니다. 이를 위해 운영체제는 자원을 관리하고, 프로세스 간의 우선 순위 및 스케줄링 정책을 고려합니다.
5. 운영체제가 요청된 작업을 완료하면, 프로세스는 다시 실행 가능한 상태로 전환되어 해당 작업을 계속합니다.
6. 프로세스는 할당된 자원을 사용하여 원하는 작업을 수행하고, 시스템 콜을 호출한 이후에는 보통 이전 상태로 돌아갑니다.


<br/>
<h2>🤔 시스템 콜의 유형에 대해 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- 시스템 콜의 유형은 자원을 할당 받았을 때와 할당 받지 못했을 때로 나뉩니다.(표현하는 단어가 생각나지 않습니다.) ❌ 완벽히 틀렸습니다. ❌

<h3>GPT의 대답</h3>

- 프로세스 제어, 파일 조작, 장치 관리, 파일 시스템, 네트워킹, 메모리 관리, 보안 시스템 콜이 존재합니다.

<br/>
<h2>🤔 운영체제의 Dual Mode 에 대해 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- 시스템 콜의 유형은 자원을 할당 받았을 때와 할당 받지 못했을 때로 나뉩니다.(표현하는 단어가 생각나지 않습니다.) ❌ 완벽히 틀렸습니다. ❌

<h3>GPT의 대답</h3>

- 운영체제의 Dual Mode는 컴퓨터 시스템에서 사용자 모드와 커널 모드라는 두 가지 모드를 지원하는 기능을 말합니다.

- 사용자 모드는 응용 프로그램이 실행되는 모드로, 일반적인 프로세시의 동작 범위를 의미합니다. 사용자 모드에서는 제한된 자원에 접근하고, 일부 민감한 명령어나 하드웨어에 직접적인 접근이 제한됩니다.

- 반면, 커널 모드는 운영체제의 핵심 부분인 커널이 동작하는 모드로, 시스템의 모든 자원과 기능에 접근할 수 있습니다. 커널 모드에서는 시스템 콜과 같은 특권 명령어를 실행하거나 하드웨어를 직접적으로 제어할 수 있습니다.

🍎 Dual Mode 사용 이유

- **Dual Mode의 주요 목적은 운영체제가 시스템 자원을 보호하고, 응용 프로그램이 실수나 악의적인 동작으로 시스템을 손상시키지 못하도록 제한하는 것입니다.** 응용 프로그램은 사용자 모드에서 실행되며, 시스템 자원에 접근하기 위해 시스템 콜을 호출하여 커널 모드로 전환됩니다. 

<br/>
<h2>🤔 서로 다른 시스템 콜을 어떻게 구분할 수 있을까요?</h2>

<h3>📝 나의 대답</h3>

- PID를 통해 구분할 수 있습니다.

<h3>GPT의 대답</h3>

- PID는 프로세스를 식별하는 데 사용되는 고유한 식별자이며, 시스템 콜은 프로세스가 운영체제에 요청하는 인터페이스입니다.

- 시스템 콜을 구분하는 주요 방법은 시스템 콜의 번호나 식별자를 사용하는 것입니다.

</div>
</details>

<details>
<summary style="font-size:20px">인터럽트</summary>
<div markdown="1">
<br/>
<h2>🤔 인터럽트가 무엇인지 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- 인터럽트란 프로세스 진행 시 운영체제에게 자원을 요청하는 트리거입니다.

<h3>GPT의 대답</h3>

- 인터럽트는 컴퓨터 시스템에서 발생하는 이벤트나 예외 상황으로, 프로세스의 정상적인 실행 흐름을 중단하고 운영체제에게 어떤 처리를 요청하는 신호입니다.

<br/>
<h2>🤔 인터럽트는 어떻게 처리하나요</h2>

<h3>📝 나의 대답</h3>

- 인터럽트는 서브 루틴을 사용해 처리합니다. 서브 루틴은 Stack의 구조를 띄고 있으며 LIFO으로 요청이 처리됩니다.

<h3>GPT의 대답</h3>

- 인터럽트 처리는 인터럽트 서비스 루틴(ISR)을 호출하여 해당 인터럽트에 대한 처리를 수행하고, 상태를 저장하고 복원하여 중단된 프로세스의 실행을 재개하는 과정으로 이루어진다는 것입니다.

<br/>
<h2>🤔 Polling 방식에 대해 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- Polling 방식은 인터럽트에 대한 처리가 끝났는지 지속적으로 확인하는 방식입니다. ❌ 완벽히 틀렸습니다. ❌

<h3>GPT의 대답</h3>

- Polling 방식은 **CPU가 주기적으로 특정 장치나 상태를 확인**하며 인터럽트나 이벤트 발생 여부를 체크하는 방식으로, 인터럽트가 발생하면 해당 인터럽트를 처리하는 과정을 수행한다는 것입니다.

<br/>
<h2>🤔 HW / SW 인터럽트에 대해 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- 모르겠습니다.

<h3>GPT의 대답</h3>

- HW 인터럽트 : 하드웨어 인터럽트는 주로 하드웨어 장치에서 발생하는 이벤트에 의해 발생합니다. 이벤트는 입출력(I/O) 장치의 데이터 전송 완료, 타이머 경과, 외부 신호 등 다양한 형태일 수 있습니다. 하드웨어 인터럽트는 컴퓨터 시스템의 하드웨어 제어러에 의해 처리됩니다.

- SW 인터럽트 : 소프트웨어 인터럽트는 소프트웨어 실행 중에 발생하는 이벤트에 의해 발생합니다. 주로 운영체제나 프로세스가 직접 발생시키는 인터럽트입니다. 소프트웨어 인터럽트는 프로세스의 특정 상황이나 요청에 의해 발생할 수 있습니다. 

</div>
</details>

<details>
<summary style="font-size:20px">프로세스</summary>
<div markdown="1">
<br/>
<h2>🤔 프로세스란 무엇인가요</h2>

<h3>📝 나의 대답</h3>

- 프로세스란 하나의 동작하는 프로그램입니다.

<h3>GPT의 대답</h3>

- 프로세스는 컴퓨터에서 실행중인 프로그램입니다. 프로세스는 OS에 의해 관리되며 프로그램의 인스턴스로서 메모리와 CPU 등의 시스템 리소스를 할당받아 실행됩니다.

- 각 프로세스는 독립적으로 실행됩니다.


<br/>
<h2>🤔 프로그램과 프로세스, 스레드의 차이에 대해 설명해 주세요.
</h2>

<h3>📝 나의 대답</h3>

- 프로세스는 컴퓨터에서 실행중인 프로그램을 의미하며 스레드는 그 안에 존재해 작업을 처리하는 역할을 수행합니다. 하나의 프로세스 내에서 여러 개의 스레드가 존재할 수 있습니다.

<h3>GPT의 대답</h3>

- 프로세스에 대한 설명은 맞습니다. 스레드는 프로세스 안에 존재하며, **프로세스의 자원을 공유**하면서 작업을 처리하는 작은 실행 단위입니다. 

- 프로세스는 최소한 하나의 스레드를 가지고 있어야 하지만, 여러 개의 스레드를 가질 수도 있습니다. 스레드는 프로세스 내에서 병렬적으로 동작하며, 서로 다른 스레드는 프로세스의 자원을 공유하면서 작업을 수행할 수 있습니다.

<br/>
<h2>🤔 스레드는 PCB를 갖고 있을까요?</h2>

<h3>📝 나의 대답</h3>

- 아니요. 스레드는 PCB를 갖고 있지 않습니다. PCB는 실행중인 프로세스에게 할당되는 고유한 값이고 스레드는 PCB ID와 같은 정보를 알고 있습니다.

<h3>GPT의 대답</h3>

- "스레드는 PCB(Process Control Block)를 직접 갖고 있는 것은 아닙니다. PCB는 실행 중인 프로세스에게 할당되는 고유한 값으로, 프로세스의 상태, 우선순위, 자원 할당 정보 등을 포함합니다. 
  
- 스레드는 프로세스 내에서 실행되는 작은 실행 단위로, 여러 개의 스레드는 하나의 프로세스의 자원을 공유하면서 병렬적으로 동작합니다. 각 스레드는 프로세스의 PCB ID와 같은 정보를 알고 있어 해당 프로세스의 자원에 접근하고 조작할 수 있습니다."

<br/>
<h2>🤔 리눅스에서, 프로세스와 스레드는 각각 어떻게 생성될까요?</h2>

<h3>📝 나의 대답</h3>

- 리눅스 환경에서 프로세스는 Root 프로세스에서 자식 프로세스의 형식으로 생성되며 스레드는 생성된 프로세스에 기본값으로 스레드가 할당됩니다.

<h3>GPT의 대답</h3>

- 리눅스에서 프로세스는 **'fork()' 시스템 콜**을 통해 생성됩니다. 이 'fork()' 함수는 호출한 프로세스의 **복사**본인 자식 프로세스를 만들며, 이 둘은 독립된 메모리 공간을 갖게 됩니다. 이후 **'exec()' 시스템 콜**을 사용하여 자식 프로세스의 **메모리에 새로운 프로그램을 로드**할 수 있습니다.

- 스레드의 생성은 'pthread_create()' 함수를 통해 이루어집니다. 이 함수를 호출하면 호출한 프로세스 내에서 새로운 스레드가 생성됩니다.

<br/>
<h2>🤔 자식 프로세스가 상태를 알리지 않고 죽거나, 부모 프로세스가 먼저 죽게 되면 어떻게 처리하나요?</h2>

<h3>📝 나의 대답</h3>

- ❗️ 모르겠습니다.

<h3>GPT의 대답</h3>

✅ 자식 프로세스가 상태를 알리지 않고 종료될 경우

- 이 경우 자식 프로세스는 'Zombie 프로세스'가 됩니다. 이는 프로세스가 종료되었지만 부모 프로세스가 그 상태 정보를 아직 회수하지 않은 상태를 말합니다. 이 정보를 회수하지 않으면 프로세스 테이블에 불필요한 정보가 계속 남아 있어, 자원 낭비를 초래할 수 있습니다. 이를 해결하기 위해 부모 프로세스는 'wait()' 또는 'waitpid()' 함수를 호출하여 자식 프로세스의 상태 정보를 회수합니다.

✅ 부모 프로세스가 먼저 종료될 경우

- 이 경우 자식 프로세스는 'Orphan 프로세스'가 됩니다. 부모 프로세스가 종료되면 그의 자식 프로세스들은 init 프로세스(프로세스 ID가 1인 프로세스)가 자동으로 채택하여 그 부모로 만듭니다. init 프로세스는 주기적으로 'wait()'를 호출하여 종료된 자식 프로세스의 상태 정보를 회수하므로, Orphan 프로세스가 Zombie 프로세스가 되는 것을 방지합니다.

<br/>
<h2>🤔 리눅스에서, 데몬프로세스에 대해 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- 데몬 프로세스란 백그라운드에서 실행되는 프로세스를 의미하며 운영체제가 구동될 수 있게 도와주는 역할을 수행합니다.

<h3>GPT의 대답</h3>

- 데몬 프로세스는 사용자와의 상호작용 없이 백그라운드에서 실행되는 프로세스로, 주로 시스템 작업이나 서비스를 관리하며 일반적으로 시스템 부팅 시에 시작되어 시스템이 종료될 때까지 계속 실행됩니다.
  
- 예를 들어, 웹 서버, FTP 서버, 데이터베이스 서버 등의 서비스는 모두 데몬 프로세스로서 동작하며, 이들은 사용자의 요청에 응답하고 필요한 작업을 수행합니다.

</div>
</details>

<details>
<summary style="font-size:20px">프로세스 주소 공간</summary>
<div markdown="1">
<br/>
<h2>🤔 프로세스 주소공간에 대해 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- 프로세스 주소 공간엔 5가지 영역이 존재합니다. 코드, 데이터, 힙, 스텍, 메모리 영역이 존재합니다. 코드 영역은 실행할 프로그램의 코드를 저장. 데이터 영역은 전역 변수와 정적 변수를 저장, 힙 영역은 인스턴스를 할당 해제하는 메모리 공간, 스택 영역은 프로그램이 자동으로 사용하는 메모리 영역입니다.

<h3>GPT의 대답</h3>

- 코드(Code) 영역: 프로세스가 실행할 코드, 즉 컴파일된 프로그램의 기계어를 저장합니다. 이 영역은 읽기 전용이므로 프로세스가 자신의 코드를 수정하는 것을 막습니다.

- 데이터(Data) 영역: 전역 변수와 정적 변수가 저장되는 영역입니다. 프로그램 시작 시 할당되며 프로그램 종료 시 소멸합니다.

- 힙(Heap) 영역: 동적 메모리 할당이 이루어지는 영역으로, 개발자에 의해 메모리 할당과 해제가 이루어집니다. 메모리 요구량이 커질수록 힙 영역은 코드 영역의 방향으로 확장됩니다.

- 스택(Stack) 영역: 지역 변수, 함수 매개변수, 반환 주소 등이 저장되는 영역입니다. 함수의 호출과 함께 할당되며, 함수의 호출이 완료되면 소멸됩니다. 메모리 요구량이 커질수록 스택 영역은 데이터 영역의 방향으로 확장됩니다.

<br/>
<h2>🤔 초기화 하지 않은 변수들은 어디에 저장될까요?</h2>

<h3>📝 나의 대답</h3>

- 초기화 하지 않은 변수들은 코드 영역에 저장됩니다. ❌ 틀렸습니다.

<h3>GPT의 대답</h3>

- 초기화 하지 않은 변수들은 데이터 영역에 존재합니다.

- 데이터 영역은 전역 변수가 존재하는 영역이지만 그 안에 초기화된 전역 변수가 위치하는 공간과 초기화 되지 않은 전역변수가 위치하는 공간이 따로 존재합니다.이를 BSS(Block Started By Symbol)영역 이라고 부릅니다.

<h3>❓ 초기화된 전역 변수와 초기화 되지 않은 전역 변수는 어떤 다른점이 있나요?</h3>

- 메모리를 효율적으로 사용할 수 있게 됩니다. 초기화되지 않은 변수의 값은 실행 도중 채워질 것이므로 변수의 값을 따로 기억할 필요가 없기 때문에 메모리 공간을 절약할 수 있습니다.

<br/>
<h2>🤔 일반적인 주소공간 그림처럼, Stack과 Heap의 크기는 매우 크다고 할 수 있을까요? 그렇지 않다면, 그 크기는 언제 결정될까요?</h2>

<h3>📝 나의 대답</h3>

- 프로세스 실행 시 기본적으로 필요한 인스턴스 공간을 제외하고 런타임 시 크기가 결정됩니다.

<h3>GPT의 대답</h3>

- 스택과 힙의 크기는 프로세스가 실행되는 동안 동적으로 변경됩니다.

<br/>
<h2>🤔 Stack과 Heap 공간에 대해, 접근 속도가 더 빠른 공간은 어디일까요?</h2>

<h3>📝 나의 대답</h3>

- 답은 Stack입니다. Stack에선 지역 변수, 함수 매개변수, 반환 주소 등을 저장해 사용하고 이를 바탕으로 Heap 공간의 인스턴스들을 사용하기 때문에 접근 속도는 Heap 보다 Stack이 더 빠릅니다. ❌ 틀린 답변입니다. ❌

<h3>GPT의 대답</h3>

- Stack 메모리는 구조상 연속적으로 구조가 고정되어 있어, 메모리 접근 속도가 힙에 비해 빠릅니다.

<br/>
<h2>🤔 프로세스 주소 공간을 분할하는 이유는 무엇인가요</h2>

<h3>📝 나의 대답</h3>

- 공간을 분리하는 이유는 각 영역의 책임과 역할이 다르기 때문입니다. 어떤 영역에서는 프로그램 실행 시 크기를 예측할 수 있는 정적 영역을 관리하고 어떤 영역에서는 동적으로 영역을 확보해 사용합니다.

- 이를 한 곳의 영역에서 관리한다면 복잡성이 늘어날 것이기 때문에 공간을 분리하는 것입니다.

<h3>GPT의 대답</h3>

- 프로세스 주소 공간을 분할하는 이유는 각 영역의 책임과 역할이 다르기 때문이며, 이는 프로그램의 복잡성을 줄여주고, 프로그램의 안정성을 높여줍니다. 또한, 각 영역은 다른 권한을 갖고 있기 때문에 **메모리 보호**를 위해서도 주소 공간을 분할합니다.

<br/>
<h2>🤔 스레드의 주소공간은 어떻게 구성되어 있을까요?</h2>

<h3>📝 나의 대답</h3>

- 스레드는 동적으로 생성되어 사용되며 프로세스의 힙, 데이터, 코드 영역을 공유해 사용합니다. 스택 영역은 독립적으로 사용합니다.

<h3>GPT의 대답</h3>

- 스레드는 동일한 프로세스 내에서 실행되며, 따라서 코드, 데이터, 힙 영역을 공유합니다. 그러나 각 스레드는 자신만의 스택 영역을 가지고 있어, 함수 호출과 반환, 지역 변수 등 스레드 별로 다른 정보를 관리할 수 있습니다.
 
- 이는 **스레드의 독립적인 실행 흐름을 보장**하는 데 중요한 역할을 합니다. 
</div>
</details>

<details>
<summary style="font-size:20px">단기, 중기, 장기 스케쥴러</summary>
<div markdown="1">
<br/>
<h2>🤔 단기, 중기, 장기 스케쥴러에 대해 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- 모르겠습니다.

<h3>GPT의 대답</h3>

✅ 장기 스케줄러 :  하드 디스크에 저장된 프로세스들 중 어떤 것을 메인 메모리에 적재할지 결정합니다. 이 스케줄러의 주된 목적은 적절한 프로세스의 조합을 통해 메모리의 효율적인 사용을 달성하는 것입니다.

✅ 중기 스케줄러 : 실행 중인 프로세스를 잠시 중지하고 하드 디스크에 다시 저장하는 스왑 인/아웃(Swapping in/out) 과정을 담당합니다.

✅ 단기 스케줄러 :  메모리에 있는 프로세스 중 어떤 것을 CPU에 할당할지를 결정합니다. 즉, 어떤 프로세스가 다음에 실행될지를 결정합니다.

<br/>
<h2>🤔 현대 OS에는 단기, 중기, 장기 스케쥴러를 모두 사용하고 있나요?</h2>

<h3>📝 나의 대답</h3>

- 네 현대 OS에는 단기, 중기, 장기 스케줄러를 사용하며 성능을 더욱 끌어 올리기 위해 캐시 메모리를 적용해 사용합니다.

<h3>GPT의 대답</h3>

- 현대의 운영 체제 대부분은 장기 스케줄러와 단기 스케줄러를 사용합니다. 이는 각각 어떤 프로세스를 메모리에 적재할지, 그리고 어떤 프로세스를 CPU에 할당할지를 결정합니다. 
  
- 중기 스케줄러는 오래된 시스템에서 스왑 작업을 관리하는 데 사용되었지만, 현대의 운영 체제에서는 대체로 사용되지 않습니다. 

<br/>
<h2>🤔 preemptive/non-preemptive 에서 존재할 수 없는 상태가 있을까요?</h2>

<h3>📝 나의 대답</h3>

- 모르겠습니다.

<h3>GPT의 대답</h3>

- 스케줄링에 있어서 preemptive와 non-preemptive는 **운영체제가 CPU를 어떻게 관리하는지**에 대한 전략을 의미합니다.

✅ Preemptive 스케줄링은 운영 체제가 현재 실행 중인 프로세스를 중지하고 다른 프로세스를 실행할 수 있는 권한을 가지고 있는 상황을 말합니다.

✅ Non-preemptive 스케줄링은 현재 실행 중인 프로세스가 자발적으로 CPU를 반환할 때만 다른 프로세스가 실행될 수 있는 상황을 말합니다.

<br/>
<h2>🤔 Memory가 부족할 경우, Process는 어떠한 상태로 변화할까요?
</h2>

<h3>📝 나의 대답</h3>

- 메모리가 부족할 경우, 만약 사용 가능한 메모리 범위를 넘어섰을 때, 프로세스는 Overflow Error를 반환하며 운영체제에 의해 강제 종료됩니다. 프로세스가 강제종료 되지 않게 내부적으로 메모리 Clean 과정이 강제로 이뤄질 수도 있습니다. 자바에서는 GC를 통해 관리됩니다.

<h3>GPT의 대답</h3>

- 메모리가 부족할 경우, 프로세스는 운영 체제에 의해 '대기' 상태로 전환될 수 있습니다. 이는 메모리가 다시 사용 가능해질 때까지 프로세스가 실행되지 않음을 의미합니다. 이런 상황은 주로 운영 체제의 스왑 메커니즘이 작동할 때 발생합니다. 

- 또한, 만약 사용 가능한 메모리 범위를 넘어서는 동작이 시도될 경우, 프로세스는 오류를 반환하며 강제로 종료될 수 있습니다. 이외에도, 일부 프로그래밍 언어나 환경에서는 메모리 관리를 위한 별도의 메커니즘을 제공하기도 합니다. 예를 들어, 자바에서는 가비지 컬렉터(GC)를 통해 사용되지 않는 메모리를 정리합니다

<br/>
</div>
</details>

<details>
<summary style="font-size:20px">컨텍스트 스위칭</summary>
<div markdown="1">
<h2>🤔 컨텍스트 스위칭 시에는 어떤 일들이 일어나나요?</h2>

<h3>📝 나의 대답</h3>

- 컨텍스트 스위칭이 일어나면 기존에 존재하는 프로세스의 기록을 해당 프로세스의 PCB에 업데이트를 진행하고 다음 실행될 프로세스가 OS에게 시스템 콜이 발생시켜 시스템 자원을 사용할 수 있게 됩니다.

<h3>GPT의 대답</h3>

- 컨텍스트 스위칭이 일어나면, 운영 체제는 현재 실행 중인 프로세스의 상태를 PCB에 저장합니다. 그런 다음, 새롭게 실행될 프로세스의 PCB 정보를 불러와 CPU 레지스터 및 프로그램 카운터를 업데이트합니다. 이 과정은 새로운 프로세스가 이전에 중단된 지점부터 실행을 계속할 수 있게 해줍니다.
  
- 또한, 필요에 따라 메모리 관리 정보도 업데이트 될 수 있습니다. 이러한 과정을 통해 운영 체제는 다수의 프로세스나 스레드가 동시에 실행되는 것처럼 보이게 할 수 있습니다.

<br/>
<h2>🤔 프로세스와 쓰레드는 컨텍스트 스위칭이 발생했을 때 어떤 차이가 있을까요?</h2>

<h3>📝 나의 대답</h3>

- 차이점을 이야기 하기 전에 공통점을 먼저 말하자면 문맥을 교환한다는 것과 교환 시 오버헤드가 발생한 다는 것입니다. 차이점은 스레드 스위칭은 프로세스 내부에서 발생한다는 것입니다.

<h3>GPT의 대답</h3>

- 프로세스 컨텍스트 스위칭과 스레드 컨텍스트 스위칭 사이의 주요 차이점은 **저장하고 복구해야 하는 컨텍스트의 양과 복잡도**에 있습니다.

- 프로세스 컨텍스트 스위칭에서는 프로세스의 전체 상태, 즉 모든 레지스터 값, 메모리 맵, 열린 파일 디스크립터 등을 저장하고 복구해야 합니다. 
  
- 그러나 스레드 컨텍스트 스위칭에서는 스레드가 속한 프로세스의 코드, 데이터, 힙 등을 공유하기 때문에, 보통 스택 포인터와 프로그램 카운터, 레지스터 값 등만 저장하고 복구하면 됩니다.

<br/>
<h2>🤔 컨텍스트 스위칭이 발생할 때, 기존의 프로세스 정보는 커널스택에 어떠한 형식으로 저장되나요?</h2>

<h3>📝 나의 대답</h3>

- 커널 스택에 PCB 주소 값을 갖는 형태로 저장됩니다.

<h3>GPT의 대답</h3>

- 컨텍스트 스위칭이 발생할 때, 현재 실행 중인 프로세스의 상태는 그 프로세스의 프로세스 제어 블록(PCB)에 저장됩니다. PCB는 프로세스의 중요한 정보를 포함하고 있습니다. 예를 들어 프로세스 상태, 프로그램 카운터, CPU 레지스터 값, 스케줄링 및 메모리 정보 등이 여기에 포함됩니다.


- 이 PCB는 커널 메모리 공간에 저장되며, 컨텍스트 스위칭이 발생할 때 이 정보를 사용하여 이전의 프로세스 상태를 복원합니다.

<br/>
<h2>🤔 컨텍스트 스위칭은 언제 일어날까요?</h2>

<h3>📝 나의 대답</h3>

- 컨텍스트 스위칭은 기존에 컴퓨터 리소스를 점유하고 있는 프로세스를 교체할 때 발생합니다. ❌ 틀린 답변입니다.

<h3>GPT의 대답</h3>

- 프로세스가 I/O 작업을 요청하면 해당 작업이 완료될 때까지 프로세스는 CPU에서 제거되고 다른 프로세스가 CPU를 사용하게 됩니다.

- 운영체제의 스케줄러가 특정 프로세스의 CPU 사용 시간을 할당한 시간을 모두 사용했을 때 발생합니다.

- 또한, 프로세스가 종료되거나, 자식 프로세스를 생성하거나, 시그널이나 인터럽트가 발생할 경우에도 컨텍스트 스위칭이 발생합니다.
</div>
</details>

<details>
<summary style="font-size:20px">프로세스 스케줄링 알고리즘</summary>
<div markdown="1">
<br/>
<h2>🤔 프로세스 스케줄링 알고리즘엔 어떤 것이 있나요?</h2>

<h3>📝 나의 대답</h3>

- 프로세스 스케줄링은 다양한 알고리즘이 존재합니다. SJF, LJF, RR 알고리즘 기법이 존재하며 지향하는 목표는 효율적인 작업 처리입니다. 알고리즘을 분류화할 때 선점형 스케줄링과 비선점 스케줄링으로 나눌 수 있습니다.

- 선점형 스케줄링은 선점하고 있는 프로세스의 사용을 중단하고 다른 프로세스가 CPU를 사용할 수 있는 스케줄링이고 비선점형 스케줄링은 하나의 프로세스의 CPU bound 작업이 끝날 때까지 자원을 점유하는 스케줄링 기법입니다.

<h3>GPT의 대답</h3>

- 선점형 스케줄링은 우선순위가 더 높은 프로세스가 실행되어야 할 때 현재 실행 중인 프로세스를 중지하고 CPU를 빼앗는 방식입니다.

- 반면 비선점형 스케줄링은 현재 프로세스가 완료될 때까지 CPU를 점유하게 됩니다. 이렇게 다양한 스케줄링 알고리즘들은 상황에 따라 적절하게 사용되어 시스템의 성능과 효율성을 높입니다.

<br/>
<h2>🤔 RR을 사용할 때, Time Slice에 따른 trade-off를 설명해 주세요?</h2>

<h3>📝 나의 대답</h3>

- RR(Round Robin) 알고리즘을 사용하면 시분할을 통해 CPU 사용을 관리합니다. RR 사용 시 시분할 시간에 따라 문제가 발생할 수 있습니다.

- 먼저, 시분할 시간이 짧다면 작업을 수행하는 시간보다 프로세스를 교체하는 오버헤드 비용이 커져 성능이 좋지 않을 수 있습니다.

- 다음으로 시분할 시간이 길다면 비선점형 스케줄링인 First Job First와 같은 상황이 발생합니다. 이는 선점형 스케줄링을 사용해 얻을 수 있는 장점을 사용하지 못합니다.

<h3>GPT의 대답</h3>

- RR(Round Robin) 알고리즘을 사용하면 시분할을 통해 CPU 사용을 관리합니다. 하지만 이 때 시간 할당량(time slice) 크기에 따라 여러 trade-offs가 발생할 수 있습니다.

- 시간 할당량이 너무 작다면, 실제 작업 수행 시간보다 프로세스 교체(Context Switching)로 인한 오버헤드가 높아질 수 있습니다. 이는 프로세스를 너무 자주 전환하게 되어, 전환 시 발생하는 비용이 시스템의 전반적인 성능을 저하시키게 됩니다.

- 반대로 시간 할당량이 너무 크다면, 일부 프로세스가 과도하게 오래 기다리게 될 수 있습니다. 이는 특히, I/O 바운드 작업을 수행하는 프로세스들에게 문제가 될 수 있습니다. 이 경우, 프로세스는 대부분의 시간을 I/O 대기 상태로 보내고, 그 사이에 CPU는 대부분 시간을 유휴 상태로 보내게 됩니다. 이렇게 되면 **RR 알고리즘의 기본적인 목표인 CPU 사용의 공정성과 효율성을 해치게 됩니다**.

<br/>
<h2>🤔 싱글 스레드 CPU 에서 상시로 돌아가야 하는 프로세스가 있다면, 어떤 스케쥴링 알고리즘을 사용하는 것이 좋을까요? 또 왜 그럴까요?</h2>

<h3>📝 나의 대답</h3>

- 선점형 스케줄링 알고리즘이 좋다고 생각합니다. 이유는 CPU를 효율적으로 사용할 수 있기 때문입니다. 물론 컨텍스트 스위칭 비용이 발생하지만 여럿 프로세스에게 CPU를 골고루 나눠줌으로써 사용자에게 적합한 응답시간을 제공해줄 수 있기 때문입니다.

<h3>GPT의 대답</h3>

- 상시로 돌아가야 하는 프로세스가 있는 경우, 선점형 우선순위 스케줄링(Priority Scheduling) 알고리즘이 적합합니다. 이 알고리즘은 각 프로세스에 우선순위를 부여하며, 우선순위가 높은 프로세스가 CPU를 먼저 사용하게 됩니다. 이런 방식으로, 상시로 돌아가야 하는 프로세스에게 높은 우선순위를 부여함으로써 해당 프로세스가 CPU 사용을 보장받을 수 있습니다.

<br/>
<h2>🤔 동시성과 병렬성의 차이에 대해 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- 동시성은 하나의 작업을 여럿으로 나눠 실행하는 것이고 병렬성은 여러 작업을 동시에 수행하는 것을 의미합니다. ❌ 동시성 설명은 틀렸습니다.

- 동기와 동시를 잘 구분해야합니다. 또한, 동시성과 병렬성은 같지 않습니다.

<h3>GPT의 대답</h3>

- 동시성(Concurrency)은 논리적으로 여러 작업이 동시에 실행되는 것처럼 보이도록 만드는 것을 말합니다. 이는 실제로는 **하나의 CPU가 여러 작업을 번갈아 가며 수행**하는 것입니다. 

- 병렬성(Parallelism)은 실제로 여러 작업을 동시에 수행하는 것을 말합니다. 병렬 처리를 위해서는 여러 개의 CPU 또는 코어가 필요하며, 각각의 CPU나 코어가 독립적으로 작업을 수행합니다.

<br/>
<h2>🤔 타 스케쥴러와 비교하여, Multi-level Feedback Queue는 어떤 문제점들을 해결한다고 볼 수 있을까요?</h2>

<h3>📝 나의 대답</h3>

- 큐마다 책임을 나눠 각각 다른 시분할을 적용해 선점 알고리즘을 수행할 수 있습니다. ❌ 틀린 답변입니다.

<h3>GPT의 대답</h3>

- Multi-level Feedback Queue(MFQ)는 여러 대기열을 사용하고, 각 대기열이 다른 우선순위를 가지도록 설계된 스케줄링 알고리즘입니다. 이 알고리즘은 기본적으로 **CPU-bound 프로세스와 I/O-bound 프로세스를 효과적으로 구별하고 관리함**으로써 복잡한 스케줄링 요구사항을 만족시킵니다.

- CPU Bound 작업이 많다면 우선순위를 낮게 설정하고 I/O Bound 작업이 많다면 우선순위를 높게 책정합니다. 이는 각각의 Bound 작업에 더 많은 시간을 사용할 수 있는 이점을 줍니다.

</div>
</details>

<details>
<summary style="font-size:20px">뮤텍스와 세마포어의 차이점</summary>
<div markdown="1">
<br/>
<h2>🤔 뮤텍스와 세마포어의 차이점</h2>

<h3>📝 나의 대답</h3>

- 뮤텍스와 세마포어는 임계 영역에서 안전하게 공유된 자원을 관리하기 위한 기술입니다. 차이점은 모르겠습니다.

<h3>GPT의 대답</h3>

- 뮤텍스(Mutex)와 세마포어(Semaphore)는 모두 병렬 처리에서 동시성 제어를 위한 동기화 기법이지만, 이들의 용도와 작동 방식에는 중요한 차이가 있습니다.

- 뮤텍스는 상호 배제를 위한 도구로서, 주로 단일 자원에 대한 접근을 제어하는 데 사용됩니다. 뮤텍스를 획득한 스레드만이 해당 자원에 접근할 수 있으며, 다른 스레드들은 뮤텍스가 해제될 때까지 기다려야 합니다.

- 세마포어는 여러 개의 동일한 자원에 대한 접근을 제어하는 데 사용됩니다. 세마포어의 값은 동시에 접근할 수 있는 자원의 개수를 나타내며, 이 값이 0 이하가 되면 접근을 기다리는 스레드들은 대기 상태가 됩니다. 따라서 세마포어는 뮤텍스보다 더 일반적인 동기화 도구로 볼 수 있습니다. ex) 프린터, 파일 시스템, DB 특정 레코드

</div>
</details>

<details>
<summary style="font-size:20px">DeadLock</summary>
<div markdown="1">
<br/>
<h2>🤔 DeadLock에 대해 설명해 주세요</h2>

<h3>📝 나의 대답</h3>

- DeadLock은 프로세스가 자원을 얻지 못해 다음 처리를 하지 못하는 상태입니다.

<h3>GPT의 대답</h3>

- DeadLock은 둘 이상의 프로세스나 스레드가 서로 다른 프로세스가 소유한 자원을 대기하며 무한 대기 상태에 빠지는 현상을 이야기합니다. 이런 상황은 시스템 자원의 비효율적인 사용을 초래하고 시스템 성능을 저하시킵니다.

<br/>
<h2>🤔 Deadlock 이 동작하기 위한 4가지 조건에 대해 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- DeadLock은 상호 배제, 비선점, 점유와 대기, 순환 대기 네 조건을 모두 만족해야 발생합니다.

<h3>GPT의 대답</h3>

✅ 상호 배제 : 자원은 한 번에 한 프로세스에 의해서만 사용될 수 있습니다.

✅ 점유와 대기 : 프로세스는 최소한 하나의 자원을 점유하고 있으면서, 다른 프로세스가 이미 점유하고 있는 추가적인 자원들을 기다리는 상태입니다.

✅ 비선점 : 자원을 이미 점유하고 있는 프로세스로부터 그 자원을 강제롤 뺏어올 수 없습니다.

✅ 순환 대기 : 서로가 서로의 자원을 기다리는 프로세스들의 원형을 이루는 것입니다.

<br/>
<h2>🤔 그렇다면 3가지만 충족하면 왜 Deadlock 이 발생하지 않을까요?</h2>

<h3>📝 나의 대답</h3>

- 하나라도 만족하지 않는다면 프로세스가 자원을 얻을 수 있기 때문입니다.

<br/>
<h2>🤔 어떤 방식으로 예방할 수 있을까요?</h2>

<h3>📝 나의 대답</h3>

- 데드락을 예방할 수 있는 방법은 자원을 공유 자원으로 변경해 여러 프로세스에서 사용하게 만들 수 있고 선점을 통해 강제로 자원을 뺐으며 순환 대기를 끊는 구조를 사용하면 됩니다.

<h3>GPT의 대답</h3>

- 상호 배제(Mutual Exclusion): 데드락의 발생 조건 중 하나인 상호 배제를 완화하기 위해, 가능한 한 공유 가능한 자원을 사용하도록 시스템을 설계합니다. 하지만, 모든 자원이 공유 가능한 것은 아니므로, 이 방법은 한정적입니다.

- 점유와 대기(Hold and Wait): **프로세스가 자원을 요청할 때, 이미 보유하고 있는 자원을 모두 해제한 상태**에서만 요청을 하게 하는 방법이 있습니다. 이렇게 하면, 한 프로세스가 자원을 점유한 상태에서 다른 자원을 대기하는 상황을 막을 수 있습니다.

- 비선점(No Preemption): 비선점 조건을 완화하기 위해서는, 자원을 사용 중인 프로세스로부터 필요에 따라 자원을 회수하는 방법이 있습니다. 이 방식은 운영체제가 특정 프로세스에게서 자원을 선점하고, 다른 프로세스에게 그 자원을 재할당할 수 있는 기능을 제공해야 합니다.

- 순환 대기(Circular Wait): **자원 할당 순서를 정해** 놓는 방법으로 순환 대기를 막을 수 있습니다. 각 자원에 고유한 번호를 부여하고, 프로세스가 번호 순서대로만 자원을 요청하도록 하는 것입니다.

<br/>
<h2>🤔 왜 현대 OS는 Deadlock을 처리하지 않을까요?</h2>

<h3>📝 나의 대답</h3>

- 현대 OS는 기본적으로 선점형 스케줄링 방식을 택하고 있기 때문에 교착상태 발생 조건인 비선점을 만족시키지 않습니다. 따라서 DeadLock을 따로 처리하지 않습니다.

<h3>GPT의 대답</h3>

- 현대 운영 체제가 데드락을 직접 처리하지 않는 주요한 이유는 데드락 예방이 많은 비용을 요구하고, 실제 시스템에서는 데드락이 그렇게 자주 발생하지 않기 때문입니다. 
  
- 따라서, 대부분의 운영 체제는 데드락 회피나 데드락 검출과 복구에 더 집중합니다. 또한, 데드락을 완전히 피하려면 많은 비용이 필요하므로, 실제로는 시스템 자원을 효율적으로 사용하고 데드락 발생 확률을 최소화하는 방향으로 설계됩니다.

- 데드락 회피는 시스템이 자원 할당 상태를 주시하며 데드락이 발생할 가능성이 있는 상황을 미리 예측하여 회피하는 방법입니다. 이러한 방식은 시스템의 자원 이용률을 낮출 수 있기 때문에 효율적인 자원 할당이 필요한 실시간 시스템에서는 적합하지 않을 수 있습니다.

<br/>
<h2>🤔 Wait Free와 Lock Free를 비교해 주세요.</h2>

<h3>📝 나의 대답</h3>

- Wait Free는 대기 상황에서 벗어나는 것을 의미합니다.

- Lock Free는 잠금에 대해서 벗어나는 것을 의미합니다.

<h3>GPT의 대답</h3>

✅ Wait-Free: Wait-free 알고리즘은 모든 쓰레드가 계속 진행하고 일정 시간 내에 작업을 완료하는 것을 보장하는 알고리즘입니다. 이는 각 쓰레드가 어떤 다른 쓰레드에도 의존하지 않는다는 것을 의미하며, 따라서 별도의 대기 상태 없이 쓰레드의 실행이 보장됩니다.

✅ Lock-Free: Lock-free 알고리즘은 한 번에 하나 이상의 쓰레드가 진행하고 시스템 전체가 블로킹되지 않는 것을 보장하는 알고리즘입니다. 즉, 어떤 쓰레드가 중지되더라도 시스템 전체가 멈추지 않고 다른 쓰레드가 계속 작업을 수행할 수 있습니다. (I/O Bound 작업 시 내가 했던게 이거구나)

</div>
</details>

<details>
<summary style="font-size:20px">프로그램 실행 과정</summary>
<div markdown="1">
<br/>
<h2>🤔 프로그램이 컴파일 되어, 실행되는 과정을 간략하게 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- 프로그램을 실행시키면 보조 메모리에서 주 메모리로 프로그램을 적재 시킵니다. 이후 컴파일을 거치고 프로그램이 실행됩니다.

<h3>GPT의 대답</h3>

- 프로그래머가 특정 프로그래밍 언어로 소스 코드를 작성합니다. 이 소스 코드는 컴파일러에 의해 기계어로 번역됩니다. 번역된 코드는 링커에 의해 필요한 라이브러리와 함께 최종 실행 파일을 생성합니다. 이 실행 파일은 로더에 의해 메모리에 적재되고, CPU가 이를 수행하여 프로그램이 실행됩니다.

<br/>
<h2>🤔 링커와, 로더의 차이에 대해 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- 링커는 컴파일된 코드와 그 코드에 필요한 라이브러리를 엮는 역할을 수행합니다.

- 로더는 만들어진 실행 파일을 메모리에 적재하는 역할을 수행합니다.

<br/>
<h2>🤔 컴파일 언어와 인터프리터 언어의 차이에 대해 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- 컴파일 언어는 소스 코드 전체를 컴파일하여 기계어로 번역하는 것이고 인터프리터 언어는 각 줄마다 해석해 기계어로 번역하는 것입니다.

<h3>GPT의 대답</h3>

- 컴파일 언어는 실행 전에 전체 코드가 기계 코드로 번역되므로 실행 속도가 빠르다는 이점이 있습니다. 반면에 코드를 수정한 후에는 다시 전체를 컴파일해야 하는 단점이 있습니다.

- 인터프리터 언어는 각 줄을 실행 시점에 해석하고 실행하므로 코드를 작성하면서 실시간으로 테스트와 디버깅을 할 수 있습니다. 그러나 이런 방식 때문에 실행 속도가 느리다는 단점이 있습니다.

<br/>
<h2>🤔 JIT에 대해 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- JIT은 인터프리터 언어에서 사용하는 기법이며 속도가 느린 것을 보완하기 위해 내부적으로 컴파일을 사용하는 것입니다.

<h3>GPT의 대답</h3>

- JIT 컴파일러는 프로그램을 실제 실행하는 도중에 해당 코드를 기계어로 변환합니다. 이 방식은 인터프리터의 실시간 반응성과 컴파일러의 효율성을 결합하여, 필요할 때만 특정 코드를 컴파일하여 실행 속도를 향상시키는데 목표를 두고 있습니다. 

- 동일한 코드가 반복적으로 실행되는 경우, JIT 컴파일러는 한 번 컴파일된 코드를 캐시에 저장하여 재사용함으로써 성능을 향상시키는 데 기여합니다.

<br/>
<h2>🤔 본인이 사용하는 언어는, 어떤식으로 컴파일 및 실행되는지 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- 자바는 실행 시 오브젝트 파일을 생성합니다. 생성 후 JVM에서 해석해 기계어로 변경합니다. 변경 후 링킹과 로더를 거쳐 실행합니다. ❌ 틀렸습니다.

<h3>GPT의 대답</h3>

- Java는 소스 코드(.java 파일)를 컴파일러를 통해 바이트 코드(.class 파일)로 컴파일합니다. 이 바이트 코드는 중간 언어로 JVM이 이해할 수 있는 형태이며, 여러 플랫폼에서 동일하게 실행될 수 있습니다.

-  바이트 코드는 실행 시 JVM에 의해 실시간으로 기계어로 변환(인터프리팅 또는 JIT 컴파일)되어, 이 변환된 코드가 실제로 컴퓨터의 CPU에 의해 실행됩니다.
</div>
</details>

<details>
<summary style="font-size:20px">IPC</summary>
<div markdown="1">
<br/>
<h2>🤔 IPC가 무엇이고, 어떤 종류가 있는지 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- IPC는 프로세스 사이 데이틀 주고 받을 수 있는 통신 방법입니다. 어떤 종류가 있는지는 모르겠습니다.

<h3>GPT의 대답</h3>

- 데이터를 주고 받을 수 있는 방법은 파이프, 메시지 큐, 공유 메모리, 소켓, 시그널 등이 있습니다.

<br/>
<h2>🤔 Shared Memory가 무엇이며, 사용할 때 유의해야 할 점에 대해 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- 공유 메모리는 두 개 이상의 프로세스가 같은 메모리 세그먼트를 공유하는 방법입니다. 공유되는 자원이기 때문에 데이터 일관성이 문제가될 수 있습니다.

<br/>
<h2>🤔 메시지 큐는 단방향이라고 할 수 있나요</h2>

<h3>📝 나의 대답</h3>

- 네 단방향으로 말할 수 있습니다. 양방향으로 구성할 수 있지만 단방향으로 구성해 사용하는 것이 좋습니다. 이유는 작업 처리 범위 때문입니다. 양방향으로 만든다면 작업의 범위가 넓어져 변경의 여파가 여러 곳에서 발생하게 됩니다.

- 따라서, 단방향으로 구성하되 보상 메커니즘을 도입해 피드백으로 처리하는 것이 좋습니다.
</div>
</details>

<details>
<summary style="font-size:20px">Thread Safe</summary>
<div markdown="1">
<br/>
<h2>🤔 Thread Safe 하다는 것은 어떤 의미인가요?</h2>

<h3>GPT의 대답</h3>

- Thread Safe란 멀티 스레드 환경에서 여러 스레드가 동시에 데이터에 접근하더라도 프로그램의 실행 결과가 올바르게 나오는 것을 이야기합니다.

<br/>
<h2>🤔 Thread Safe 를 보장하기 위해 어떤 방법을 사용할 수 있나요</h2>

<h3>📝 나의 대답</h3>

- 뮤텍스와 세마포어 등 동기화 기법을 임계 지역에 사용해 Thread Safe하게 만들 수 있습니다.

<h3>GPT의 대답</h3>

- 또한, 락(locking)을 이용해 임계 영역을 보호하거나, 원자적 연산(atomic operations)을 이용해 공유 자원의 동시 수정을 방지할 수 있습니다. 더불어, 모니터나 조건 변수(condition variables)를 이용한 동기화나, 특정 프로그래밍 언어에서 제공하는 도구들, 예를 들면 자바의 synchronized 키워드 등을 사용할 수도 있습니다.

<br/>
<h2>🤔 Peterson's Algorithm 이 무엇이며, 한계점에 대해 설명해 주세요.</h2>

<h3>GPT의 대답</h3>

✅ Peterson's Algorithm은 두 개의 프로세스가 공유 자원을 동시에 사용하지 않도록 하는 mutual exclusion 솔루션입니다. 하지만 이 알고리즘은 오직 두 개의 프로세스에 대해서만 동작하며, 세 개 이상의 프로세스에 대해 확장하기는 어렵습니다.

✅ 또한, 이 알고리즘은 프로세스들이 교대로 공유 자원에 접근하고, 서로를 믿고 있는 상태에서만 제대로 작동합니다. 이러한 이유로 인해, Peterson's Algorithm은 실제 시스템에서는 잘 사용되지 않습니다.

<br/>
<h2>🤔 Race Condition 이 무엇인가요?</h2>

<h3>📝 나의 대답</h3>

- 임계 구역을 뜻하며 동시성 프로그래밍에서 공유되는 자원이 존재하는 영역을 뜻합니다. 해당 영역에서 공유 자원을 잘 관리하지 못한다면 데이터 일관성이 깨질 수 있습니다. ❌ 틀렸습니다. ❌

<h3>GPT의 대답</h3>

- Race Condition은 **두 개 이상의 프로세스나 스레드가 공유 자원에 동시에 접근하려 할 때 발생하는 상황**입니다. 이러한 동시 접근이 제어되지 않으면, 프로그램의 결과는 실행 순서에 따라 달라질 수 있습니다. 이로 인해 예상하지 못한 결과나 버그가 발생할 수 있으므로, 공유 자원에 접근하는 코드는 잘 동기화되어야 합니다.
</div>
</details>

<details>
<summary style="font-size:20px">Thread Pool, Monitor, Fork-Join</summary>
<div markdown="1">
<br/>
<h2>🤔 Thread Pool, Monitor, Fork-Join에 대해 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- Thread Pool은 Thread을 미리 생성해 프로세스 실행 시 Thread 생성 비용을 줄이고 속도를 높이기 위해 사용됩니다.

<h3>GPT의 대답</h3>

✅ Thread Pool : Thread Pool은 여러개의 Thread을 미리 생성해놓는 기법입니다. 이를 통해 새로운 작업에 대한 요청이 있을 때마다 Thread을 생성하는 비용을 줄이고, 시스템의 부하를 관리할 수 있습니다. 또한 사용되지 않는 Thread은 Pool에 보관되며, 새로운 요청이 발생하면 이 Pool에서 Thread를 꺼내 재사용합니다.

✅ Monitor : 모니터 내의 임계 영역은 오직 하나의 스레드만 실행할 수 있도록 보호되며, 다른 스레드들은 그 영역에 접근할 수 없습니다. 이를 통해 상호 배제(mutual exclusion)가 달성됩니다. 조건 변수는 스레드가 특정 조건을 만족할 때까지 대기하고 신호를 기다리는 데 사용됩니다.

✅ Fork-Join : Fork-Join은 병렬 컴퓨팅 모델 중 하나로, 하나의 작업을 여러 하위 작업으로 분할('Fork')한 다음, 이 하위 작업들을 병렬로 실행하고, 그 결과를 다시 합칩니다('Join'). 이를 통해 병렬 처리를 가능하게 하며, 멀티코어나 다중 프로세서 환경에서 효과적으로 작업을 분산시킬 수 있습니다.
</div>
</details>

<details>
<summary style="font-size:20px">캐시 메모리 및 메모리 계층성</summary>
<div markdown="1">
<br/>
<h2>🤔 캐시 메모리 및 메모리 계층성에 대해 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- 캐시 메모리는 메모리 계층 상단인 메모리와 CPU 사이에 존재하며 더 빠른 성능을 위해 사용됩니다. 많이 사용되는 데이터들을 캐시 메모리에 저장해 사용해 주, 보조 메모리를 참조하지 않기 때문에 좋습니다. 성능이 향상됩니다. 메모리 계층 존재 이유는 데이터의 성질 때문에 계층으로 분리해 사용합니다.

- 보조 메모리엔 휘발되지 않은 데이터가 주 메모리에는 실행되는 데이터를 저장하고 캐시 메모리는 빠른 성능을 위해 사용됩니다.

<h3>GPT의 대답</h3>

- 캐시 메모리는 CPU와 주 메모리 사이에 위치한 높은 속도의 메모리로, 주로 자주 사용되는 데이터나 명령어들을 빠르게 접근하기 위해 사용됩니다. 이를 통해 CPU가 주 메모리를 직접 참조하는 것보다 더 빠르게 데이터에 접근할 수 있습니다.

- 메모리 계층성은 컴퓨터 시스템 내에서 다양한 메모리 기술을 이용해 데이터를 저장하고, 처리 속도와 저장 용량, 비용 등의 트레이드 오프를 효율적으로 관리하는 구조를 말합니다.

<br/>
<h2>🤔 L1, L2 캐시에 대해 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- L1 캐시는 L2 캐시보다 CPU에 더 가까이 붙어 있습니다. 저장 데이터의 종류가 다르다는 점이 두 캐시의 차이점 입니다.

<h3>GPT의 대답</h3>

- 일반적으로 L1 캐시는 코어 당 하나씩 있으며, 대부분 **데이터 캐시와 명령어 캐시로 분리**되어 있습니다. 데이터 캐시는 메모리에서 로드된 데이터를, 명령어 캐시는 실행할 명령어를 저장합니다.

- L2 캐시는 L1 캐시보다는 느리지만 용량이 더 큽니다. 때문에 L1 캐시에서 찾지 못한 데이터와 명령어를 저장하며, L1 캐시의 대비책 역할을 합니다.

<br/>
<h2>🤔 캐시에 올라오는 데이터는 어떻게 관리되나요?</h2>

<h3>📝 나의 대답</h3>

- 캐시에서 다루는 데이터는 참조 횟수 또는 최신으로 참조한 데이터로 관리됩니다.

- 두 개의 관리 모두 캐시 히트를 높이기 위해 사용합니다.

<h3>GPT의 대답</h3>

- LRU(Least Recently Used) 알고리즘은 가장 오랫동안 사용되지 않은 데이터를 캐시에서 교체하는 방법입니다. LFU(Least Frequently Used) 알고리즘은 가장 적게 사용된 데이터를 캐시에서 교체합니다. 이러한 교체 알고리즘의 목표는 캐시 히트률을 최대화하는 것입니다.

<br/>
<h2>🤔 캐시간의 동기화는 어떻게 이루어지나요?</h2>

<h3>GPT의 대답</h3>

✅ 각 코어 또는 프로세서는 자신의 캐시에 데이터를 저장하므로, 한 코어에서 메모리의 특정 부분을 수정하면 다른 코어의 캐시에는 이전 버전의 데이터가 남아 있을 수 있습니다. 이것을 캐시 일관성 문제라고 부릅니다.

✅ 이 문제를 해결하기 위한 방법 중 하나는 캐시 일관성 프로토콜을 사용하는 것입니다. 대표적인 예로 **MESI (Modified, Exclusive, Shared, Invalid) 프로토콜**이 있습니다. 이 프로토콜은 각 캐시 라인의 상태를 추적하며, 이 상태에 따라 메모리를 어떻게 처리할지 결정합니다. 예를 들어, 한 코어가 메모리의 특정 부분을 수정하면, 해당 캐시 라인의 상태는 '수정됨(Modified)' 상태로 변경되고, 다른 코어의 동일한 캐시 라인은 '무효(Invalid)' 상태로 표시됩니다.

✅ 수정 가능성이 있는 데이터가 사용 중인 경우, 다른 캐시들은 해당 데이터에 대한 액세스를 차단하고, 수정이 완료되고 해당 데이터가 메모리와 동기화될 때까지 기다립니다.

<br/>
<h2>🤔 캐시 메모리의 Mapping 방식에 대해 설명해 주세요.</h2> 

<h3>GPT의 대답</h3>

✅ 직접 매핑(Direct Mapping): 주 메모리의 각 블록이 캐시 메모리의 특정 위치에만 매핑되는 방식입니다. 구현이 간단하고 빠르지만, 서로 다른 주 메모리 블록이 같은 캐시 라인에 매핑될 수 있어 충돌이 발생할 가능성이 있습니다.

✅ 완전 연관 매핑(Fully Associative Mapping): 주 메모리의 각 블록이 캐시의 어떤 위치에도 매핑될 수 있는 방식입니다. 이 방식은 충돌 문제를 완전히 해결하지만, 캐시 내에서 원하는 데이터를 찾기 위해 모든 캐시 라인을 검색해야 하므로 구현이 복잡하고 비용이 많이 듭니다.

✅ 집합 연관 매핑(Set Associative Mapping): 직접 매핑과 완전 연관 매핑의 중간 형태로, 캐시를 여러 '집합'으로 나누고, 각 주 메모리 블록이 특정 캐시 집합 내의 어느 위치에나 매핑될 수 있는 방식입니다. 이 방식은 직접 매핑의 충돌 문제와 완전 연관 매핑의 복잡성 사이에서 균형을 찾으려고 합니다.

<br/>
<h2>🤔 캐시의 지역성에 대해 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- 지역성은 캐시가 히트할 가능성을 높일 수 있는 조건을 이야기합니다. 지역성에는 참조 지역성과 공간 지역성이 존재하며 참조 지역성은 캐시가 참조한 데이터를 다시 참조할 확률이 높다는 것을 의미하며 공간 지역성은 참조된 데이터 주변의 데이터를 참조할 가능성을 이야기합니다.

<h3>GPT의 대답</h3>

- 시간 지역성(Temporal Locality): 시간 지역성은 최근에 참조된 데이터가 가까운 미래에 다시 참조될 가능성이 높다는 원칙입니다. 즉, 한 번 참조된 데이터는 곧 다시 참조될 확률이 높습니다. 이러한 원칙 때문에, 메모리에서 직접 데이터를 가져오는 것보다 빠른 캐시에서 데이터를 가져올 수 있습니다.

- 공간 지역성(Spatial Locality): 공간 지역성은 데이터가 메모리에 서로 가까이 배치될 경우, 한 데이터 항목을 참조하면 그 주변의 다른 데이터 항목도 곧 참조될 가능성이 높다는 원칙입니다. 이 원칙은 데이터를 메모리 블록으로 그룹화하고, 이러한 블록을 캐시에 저장하는 데 사용됩니다.

<br/>
<h2>🤔 캐시의 지역성을 기반으로, 이차원 배열을 가로/세로로 탐색했을 때의 성능 차이에 대해 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- 공간 지역성의 측면에서 가로를 탐색했을 때 성능이 좋습니다. 이유는 배열은 연속하여 데이터를 저장하기 때문에 참조된 데이터가 연속된 곳에 존재합니다. 따라서 세로를 탐색했을 때보다 가로를 탐색했을 때 유리합니다.

</div>
</details>

<details>
<summary style="font-size:20px">메모리의 연속할당 방식</summary>
<div markdown="1">
<br/>
<h2>🤔 메모리의 연속할당 방식 세 가지를 설명해주세요. (first-fit, best-fit, worst-fit)</h2>

<h3>📝 나의 대답</h3>

- first-fit : 메모리에 데이터를 저장할 때 제일 먼저 찾은 공간에 저장하는 방식입니다.

- best-fit : 저장할 데이터의 크기에 맞는 최적의 공간에 저장하는 방식입니다.

- worst-fit : x

<h3>GPT의 대답</h3>

- worst-fit : 가장 큰 가용 메모리 블록에 프로세스를 할당합니다. 이 방법은 여유 메모리 공간을 최대화하여 다른 프로세스가 메모리를 사용할 수 있도록 합니다.

<br/>
<h2>🤔 worst-fit 은 언제 사용할 수 있을까요?</h2>

<h3>📝 나의 대답</h3>

- 실행중인 프로세스의 공간이 추가적으로 필요할 때 사용할 수 있습니다.

<h3>GPT의 대답</h3>

- 여러 프로세스가 동시에 메모리를 요구하고, 이들 프로세스 중 일부가 큰 메모리 공간을 필요로 할 때 유용할 수 있습니다. 이전에 할당된 프로세스가 추가적인 메모리를 요구하는 경우에도 사용될 수 있습니다.

<br/>
<h2>🤔 성능이 가장 좋은 알고리즘은 무엇일까요?</h2>

<h3>📝 나의 대답</h3>

- Best-fit 알고리즘이 성능에 가장 좋습니다. 이유는 세 알고리즘 중 가장 공간을 효율적으로 사용할 수 있기 때문입니다. ❌ 틀렸습니다. ❌

<h3>GPT의 대답</h3>

- Best-fit 알고리즘은 매번 최적의 공간을 찾기 위해 모든 메모리 블록을 검사해야 하므로 성능 면에서는 느릴 수 있습니다.

- First-fit 알고리즘이 프로세스의 메모리 요구를 충족하는 첫 번째 가용 블록을 찾으면 바로 할당하기 때문입니다. 이로 인해 메모리를 할당하는데 필요한 시간이 줄어들게 됩니다.


</div>
</details>

 <details>
<summary style="font-size:20px">Thrashing</summary>
<div markdown="1">
<br/>
<h2>🤔 Thrashing 이란 무엇인가요?</h2>

<h3>📝 나의 대답</h3>

- Thrashing이란 페이지 교체 비용이 커져 자원이 낭비가 되는 것을 의미합니다.

<h3>GPT의 대답</h3>

- Thrashing은 시스템이 과도한 페이지 교체(swap) 작업에 매우 많은 시간을 소비하여, 실제로 유용한 작업을 수행하는 데 거의 시간을 할애하지 못하는 상태를 의미합니다.

- 메모리가 과도하게 사용되어 페이지 폴트가 너무 빈번하게 발생할 때 일어납니다. 이런 현상은 시스템의 전체 성능을 저하시키며, 때로는 시스템이 멈추거나 병목 상태에 빠지게 만들 수 있습니다.
</div>
</details>

<details>
<summary style="font-size:20px">가상 메모리</summary>
<div markdown="1">
<br/>
<h2>🤔 가상 메모리란 무엇인가요?</h2>

<h3>📝 나의 대답</h3>

- 가상 메모리란 페이징으로 데이터를 관리해 데이터를 효율적으로 사용하기 위해 사용됩니다. 가상 메모리를 사용하게 된다면 프로세스 데이터를 전부 주 메모리에 적재하지 않고 페이지 단위로 잘라 적재하게 되며 적용할 경우 동일한 메모리 크기에 많은 프로세스를 적재해 사용할 수 있습니다.

<h3>GPT의 대답</h3>

- 가상 메모리는 프로그램이 물리적 메모리보다 큰 주소 공간을 가질 수 있도록 하는 메모리 관리 기법입니다.

<br/>
<h2>🤔 가상 메모리가 가능한 이유가 무엇일까요?</h2>

<h3>📝 나의 대답</h3>

- 가상 메모리가 가능한 이유는 보조 메모리에 존재하는 데이터를 일정한 크기(페이지 사이즈)로 나눠 저장하기 때문입니다.

<h3>GPT의 대답</h3>

- 가상 메모리가 가능한 핵심적인 이유는 메모리 관리 시스템이 물리적 메모리와 디스크 사이에서 데이터를 투명하게 교환할 수 있도록 해주는 메모리 관리 알고리즘 때문입니다.

<br/>
<h2>🤔 Page Fault가 발생했을 때, 어떻게 처리하는지 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- Page Fault가 발생하면 보조 메모리에서 필요한 페이지를 찾아 메모리 블록에 저장하고 해당 데이터를 전달합니다.

<h3>GPT의 대답</h3>

1. 먼저, 운영 체제는 페이지 폴트가 유효한 참조인지(즉, 프로세스가 접근을 시도한 메모리 주소가 유효한 가상 주소인지) 확인합니다. 유효하지 않은 참조라면, 운영 체제는 일반적으로 프로세스에게 페이지 폴트 예외를 전달하고, 프로세스는 이를 처리해야 합니다.

2. 페이지 폴트가 유효한 참조인 경우, 운영 체제는 보조 메모리(일반적으로 디스크)에서 필요한 페이지를 찾습니다.

3. 필요한 페이지를 찾은 후, 운영 체제는 물리 메모리에서 사용 가능한 프레임을 찾아 그 곳에 페이지를 로드합니다. 만약 사용 가능한 프레임이 없다면, **페이지 교체 알고리즘을 사용**해 하나를 선택하고, 그 프레임의 내용을 디스크에 다시 저장한 후에 이 프레임을 사용합니다.

4. 페이지를 메모리에 로드한 후, 운영 체제는 페이지 테이블을 업데이트하여 새롭게 로드된 페이지의 위치를 반영합니다.

5. 마지막으로, 운영 체제는 원래의 메모리 접근 명령을 다시 시작하므로, 이번에는 페이지 폴트 없이 정상적으로 완료될 수 있습니다.

<br/>
<h2>🤔 페이지 크기에 대한 Trade-Off를 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- 페이지 크기가 너무 크다면 주 메모리에 메모리 파편화로 사용하지 못하는 메모리 공간이 늘어날 수 있습니다. 반면에 페이지 크기가 작다면 프로세서에서 메모리에 접근해야할 횟수가 많아져 성능에 좋지 않습니다. 따라서, 적절한 사이즈를 택해야합니다. ❌ 틀렸습니다.

<h3>GPT의 대답</h3>

- 페이지 크기가 크면, 각 페이지 내에서 사용되지 않는 메모리 공간(내부 파편화)이 증가할 수 있습니다. 예를 들어, 만약 페이지 크기가 4KB이고, 프로세스가 2KB만 필요하다면, 나머지 2KB는 낭비되게 됩니다.

- 반면에, 페이지 크기가 작다면, 페이지 테이블의 크기가 커지고, 이로 인한 메모리 관리의 overhead가 증가하게 됩니다. 즉, 페이지의 수가 많아지면 페이지 테이블이 커지고, 따라서 페이지 테이블을 관리하기 위한 비용이 증가합니다. 또한, 더 많은 페이지 교체가 발생할 수 있어서 이에 따른 성능 저하도 고려해야 합니다.

<br/>
<h2>🤔 페이지 크기가 커지면, 페이지 폴트가 더 많이 발생한다고 할 수 있나요?</h2>

<h3>📝 나의 대답</h3>

- 페이지 크기와 페이지 폴트는 관계가 없습니다. 페이지 폴트가 발생하는 것은 필요한 데이터가 주 메모리 페이지 블록에 존재하지 않기 때문에 발생하는 것이기 때문입니다. ❌ 틀렸습니다.

<h3>GPT의 대답</h3>

- 더 큰 페이지 크기를 사용하면, 주 메모리에 더 적은 페이지를 둘 수 있으므로 이론적으로 페이지 폴트가 더 자주 발생할 수 있습니다.

- 그러나, 이것은 작업 집합의 특성과 사용하는 페이지 교체 알고리즘 등 다른 요인에도 크게 의존합니다. 예를 들어, 만약 작업이 주로 연속된 메모리 영역을 사용하면 더 큰 페이지 크기가 페이지 폴트를 줄일 수 있습니다.

</div>
</details>

<details>
<summary style="font-size:20px">세그멘테이션과 페이징</summary>
<div markdown="1">
<br/>
<h2>🤔 세그멘테이션과 페이징의 차이점은 무엇인가요?</h2>

<h3>📝 나의 대답</h3>

- 두 방식 모두 외부 단편화 단점을 없애기 위해 나온 기법입니다.

- 세그멘테이션 : 모르겠습니다.

- 페이징 : 가상 메모리에 사용되는 데이터 단위이며 일정 크기로 보조 메모리에 있는 데이터를 잘라 사용합니다.

<h3>GPT의 대답</h3>

- 세그멘테이션과 페이징은 둘 다 가상 메모리를 관리하는 방식이지만, 두 기법은 메모리를 다루는 방식에서 차이점이 있습니다.

✅ 페이징은 프로세스를 일정 크기의 페이지로 나눠서 메모리에 저장합니다. 이 방식은 메모리를 일정한 크기의 블록으로 관리하기 때문에 구현이 상대적으로 단순하고 메모리를 효율적으로 사용할 수 있습니다. 그러나 이 방식은 외부 단편화를 해결하지만 내부 단편화 문제를 초래할 수 있습니다.

✅ 세그멘테이션은 프로그램을 다양한 크기의 논리적 단위인 세그먼트로 나눠서 메모리에 저장합니다. 세그멘테이션은 논리적 단위에 따라 메모리를 동적으로 할당하므로 메모리 사용 효율을 높일 수 있습니다. 그러나 이 방식은 외부 단편화 문제를 초래할 수 있습니다.

<br/>
<h2>🤔 페이지와 프레임의 차이에 대해 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- 페이지는 프로세스를 일정 크기로 나눠 저장한 저장 단위이고 프레임은 저장된 페이지와 함께 추가적인 정보를 갖는 하나의 단위입니다. 추가적인 정보로 해당 보조 메모리의 페이지 저장 위치 등이 존재합니다. ❌ 틀렸습니다.

<h3>GPT의 대답</h3>

- 페이징 시스템에서 페이지는 가상 메모리에서의 데이터 단위이며, 프레임은 물리 메모리에서의 데이터 단위입니다.

- 페이지는 프로세스가 사용하는 가상 메모리 주소 공간을 일정한 크기로 나눈 단위이며, 이 페이지들이 메모리에 적재될 때는 물리 메모리의 일정한 크기인 프레임에 저장됩니다.

<br/>
<h2>🤔 내부 단편화와, 외부 단편화에 대해 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- 메모리 단편화는 사용 가능한 메모리 공간이 있음에도 분리되어 그 공간이 존재해 낭비가 되는 것을 의미합니다.

- 외부 단편화는 주 메모리에서 프로세스와 프로세스 사이에서 발생하며 내부 단편화는 페이지 공간 안에서 발생한 단편화를 의미합니다.


<h3>GPT의 대답</h3>

- 외부 단편화는 메모리가 작은 조각으로 분할되어 사용되는 것을 의미합니다. 이로 인해 메모리가 충분히 있음에도 불구하고, 이들 조각들이 연속적이지 않아 큰 블록의 메모리를 할당할 수 없는 상황이 발생할 수 있습니다. 이를 해결하기 위해 주기적으로 메모리를 재배치하거나, 메모리를 가변적인 파티션으로 관리하는 방법을 사용합니다.

- 반면에 내부 단편화는 메모리를 일정한 크기의 블록으로 나누어 관리할 때 발생합니다. 예를 들어, 페이징 시스템에서 메모리를 일정 크기의 페이지로 분할하여 사용하게 되는데, 이때 페이지 크기보다 작은 데이터를 저장해야 하는 경우, 해당 페이지 내에 사용되지 않는 메모리 공간이 생겨나게 됩니다. 이를 내부 단편화라고 합니다. 이는 페이지의 크기를 잘 설정함으로써 최소화할 수 있습니다.

<br/>
<h2>🤔 페이지에서 실제 주소를 어떻게 가져올 수 있는지 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- 페이지에 존재하는 실제 주소는 물리적으로 나뉜 프레임의 주소를 통해 가져올 수 있습니다.

<h3>GPT의 대답</h3>

- 프로세스는 자신만의 페이지 테이블을 가지고 있습니다. 가상 주소는 페이지 번호와 오프셋으로 구성되어 있습니다. 페이지 번호는 페이지 테이블에서 해당 페이지의 물리적 주소를 찾는 데 사용되며, 이 주소에 오프셋을 더함으로써 실제 메모리 주소를 얻게 됩니다.

<br/>
<h2>🤔 어떤 주소공간이 있을 때, 이 공간이 수정 가능한지 확인할 수 있는 방법이 있나요?</h2>

<h3>📝 나의 대답</h3>

- MESI 프로토콜을 사용해 확인할 수 있습니다. ❌ 틀렸습니다.

<h3>GPT의 대답</h3>

- 페이지 테이블에는 각 페이지에 대한 정보가 담겨 있으며, 이 정보에는 해당 **페이지가 읽기 전용인지 아니면 쓰기도 허용되는지에 대한 정보**가 포함됩니다. 따라서 페이지 테이블을 확인하면 주소 공간의 수정 가능성을 알 수 있습니다.

<br/>
<h2>🤔 32비트에서, 페이지의 크기가 1kb 이라면 페이지 테이블의 최대 크기는 몇 개일까요?</h2>

<h3>GPT의 대답</h3>

✅ 32비트 주소 공간은 총 2^32 바이트의 메모리를 참조할 수 있습니다. 페이지의 크기가 1KB, 즉 2^10 바이트라면, 총 페이지 수는 2^32 / 2^10 = 2^22개가 됩니다. 따라서 페이지 테이블의 최대 크기는 2^22개 입니다.

❗️ 각 비트는 2의 거듭제곱에 해당하는 크기의 주소 공간을 표현합니다.
</div>
</details>

<details>
<summary style="font-size:20px">TLB and MMU</summary>
<div markdown="1">
<br/>
<h2>🤔 질의?</h2>

<h3>GPT의 대답</h3>

✅ TLB (Translation Lookaside Buffer)는 가상 메모리 시스템에서 페이지 번호를 물리적 메모리 주소로 변환하는 데 사용하는 캐시입니다.

✅ 가상 주소를 실제 메모리 주소로 변환하는 데 필요한 정보를 빠르게 얻을 수 있게 해서 시스템의 성능을 향상시키는 역할을 합니다.

<br/>
<h2>🤔 TLB를 쓰면 왜 빨라지나요?</h2>

<h3>📝 나의 대답</h3>

- TLB를 사용하면 페이지 테이블 까지 조회 필요 없이 많이 값을 바로 호출해 사용할 수 있기 때문에 메모리에 접근할 필요가 없습니다.

<h3>GPT의 대답</h3>

- TLB (Translation Lookaside Buffer)를 사용하면 시스템의 성능이 향상됩니다. TLB는 가상 메모리 시스템에서 페이지 번호를 물리적 메모리 주소로 빠르게 변환하는 데 사용되는 캐시입니다. 즉, 메모리 접근에 필요한 가상 주소를 실제 메모리 주소로 변환하는 데 필요한 정보를 빠르게 제공해주므로, 페이지 테이블을 직접 참조하는 것보다 메모리 접근 시간이 크게 단축됩니다.

<br/>
<h2>🤔 MMU가 무엇인가요?</h2>

<h3>GPT의 대답</h3>

- MMU는 Memory Management Unit의 약자로 CPU가 메모리에 접근할 때 가상 주소를 물리 주소로 변환하는 역할을 합니다. 이 과정에서 페이지 테이블이나 세그먼트 테이블 같은 데이터 구조를 사용하며, 이런 테이블들은 주로 메모리나 디스크에 저장됩니다.

<br/>
<h2>🤔 TLB와 MMU는 어디에 위치해 있나요?</h2>

<h3>📝 나의 대답</h3>

- TLB는 MMU 내부에 존재하며 MMU는 주 메모리와 프로세서 사이에 존재합니다.

</div>
</details>

<details>
<summary style="font-size:20px">동기화</summary>
<div markdown="1">
<br/>
<h2>🤔 동기화를 구현하기 위한 하드웨어적인 해결 방법에 대해 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- 동기화를 구현하기 위한 HW적인 해결 방법으로는 HW 내부에 존재하는 flag로 구현할 수 있습니다. ❌ 틀렸습니다.

<h3>GPT의 대답</h3>

- 하드웨어적인 동기화는 다중 스레드 또는 다중 프로세스 간의 동시 접근 및 동작을 조정하여 일관성과 안정성을 유지하기 위한 메커니즘입니다. 

- 원자적 연산(Compare And Swap), 캐시 일관성(MESI), 원자적 연산 명령어, 인터럽트 및 동기화 명령어, 펜스 및 메모리 모델을 사용해 SW 동기화 매커니즘과 함께 사용되어 효과적인 동시성 제어를 가능하게 합니다.

❓ CAS의 컨셉을 사용한 Lock-Free 알고리즘을 사용해 DB Transaction을 다룬다면 좋을텐데 그렇지 않은 이유를 설명해주세요.

- Compare and Swap과 같은 원자적 연산은 동시성 문제를 일부 해결할 수 있지만, ACID 속성을 완전히 보장하는 것은 어렵습니다.

<br/>
<h2>🤔 volatile 키워드는 어떤 의미가 있나요?</h2>

<h3>📝 나의 대답</h3>

- volatile을 사용하면 Cache에 존재하는 데이터를 사용하지 않고 메모리에 존재하는 데이터를 참조해 사용한다는 것입니다. 예를 들어 Java 환경에서 전역 변수로 선언된 데이터를 변경했을 때 최적화 과정에서 메모리를 참조하지 않고 Cache에 있는 데이터를 사용해 프로세스가 진행될 수 있습니다. 위의 경우 의도하지 않은 결과를 얻을 수 있음으로 volatile을 사용하면 좋습니다.

<h3>GPT의 대답</h3>

- volatile은 멀티스레드 환경에서 특히 중요하며, 하나의 스레드가 변경한 값을 다른 스레드가 볼 수 있게 해줍니다. 또한 volatile 변수에 대한 모든 읽기와 쓰기 작업은 원자적으로 수행됩니다.

<br/>
<h2>🤔 싱글코어가 아니라 멀티코어라면, 어떻게 동기화가 이뤄질까요?</h2>

<h3>📝 나의 대답</h3>

- 멀티 코어에서 캐시를 사용할 땐 캐시의 MESI 프로토콜을 사용해 동시성을 제어해 동기화를 이룰 수 있고 하나의 작업 단위로 쪼개 코어에 할당하게 되여 작업을 격리시켜 동기화를 이룰 수 있습니다.

<h3>GPT의 대답</h3>

- 뮤텍스(Mutex)와 세마포어(Semaphore)가 사용되며, 이를 통해 잠금과 해제를 조절하여 동기화를 달성합니다.

- 원자적 연산은 멀티코어 환경에서 동시 접근에 의한 경합 조건을 피하기 위해 사용됩니다.
</div>
</details>

<details>
<summary style="font-size:20px">페이지 교체 알고리즘</summary>
<div markdown="1">
<br/>
<h2>🤔 페이지 교체 알고리즘에 대해 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- 페이지 교체 알고리즘은 가상 메모리를 효율적으로 활용하기 위해 고안된 기법이며 가능한 Hit를 늘리기 위해 자주 사용되는 페이지를 메모리에 적재하기 위함입니다.

- 메모리에 오래 존재한 페이지를 교체하는 방법, 최근에 사용되지 않은 페이지를 교체하는 방법이 존재합니다.

<h3>GPT의 대답</h3>

- FIFO: 가장 먼저 메모리에 들어온 페이지를 가장 먼저 교체합니다.

- LRU: 가장 오래 동안 사용되지 않은 페이지를 교체합니다.

- LFU: 가장 적게 사용된 페이지를 교체합니다.

- OPT: 미래에 가장 적게 사용될 페이지를 교체합니다. (이는 실제 시스템에서는 구현이 불가능하지만, 이론적으로는 가장 효율적인 방법입니다.)

<br/>
<h2>🤔 LRU 알고리즘은 어떤 특성을 이용한 알고리즘이라고 할 수 있을까요?
</h2>

<h3>📝 나의 대답</h3>

- LRU는 시간 지역성의 특성을 이용한 알고리즘이라고 말할 수 있습니다.

<br/>
<h2>🤔 LRU 알고리즘을 구현한다면, 어떻게 구현할 수 있을까요?</h2>

<h3>📝 나의 대답</h3>

- 링크드 리스트 자료구조를 사용해 구현할 것 같습니다. 사용된 페이지를 맨 앞으로 이동 시키고 우선순위가 낮은 마지막 노드를 교체에 페이지로 사용할 것 같습니다.
  
</div>
</details>

<details>
<summary style="font-size:20px">File System</summary>
<div markdown="1">
<br/>
<h2>🤔 File Descriptor와, File System에 에 대해 설명해 주세요.</h2>

<h3>GPT의 대답</h3>

✅ File Descriptor : 운영 체제에서 파일이나 소켓 같은 시스템 리소스를 프로그램에서 다룰 때 사용하는 정수 값입니다. 특히 유닉스 계열의 운영체제에서는 이러한 파일 디스크립터를 중심으로 입출력 작업을 처리합니다. 즉, 파일을 열면 운영 체제는 해당 파일에 대한 정보를 관리하는 파일 디스크립터를 생성하고, 이후에는 입출력 작업을 파일 디스크립터를 통해 수행합니다.

✅ File System : 운영 체제가 저장 장치에 파일을 저장하거나 검색하는 방법을 정의하는 것입니다. 디렉토리 구조, 파일의 메타데이터 (생성 날짜, 수정 날짜, 접근 권한 등), 실제 데이터의 저장 방식 등을 결정하며, 다양한 유형의 파일 시스템이 존재합니다. (예: FAT32, NTFS, ext4, HFS 등)

<br/>
<h2>🤔 I-Node가 무엇인가요?</h2>

<h3>📝 나의 대답</h3>

- I-Node는 Linux에서 사용하는 파일시스템입니다. Node의 연결을 통해 파일을 관리합니다.

<h3>GPT의 대답</h3>

- I-Node는 Unix 기반의 파일 시스템에서 사용되는 데이터 구조로, 파일이나 디렉토리에 대한 중요한 정보를 저장합니다.

- I-Node는 파일 이름이나 디렉토리 구조와는 별도로 관리되며, 이는 하나의 파일이 여러 개의 이름을 가질 수 있는 하드 링크 같은 기능을 가능하게 합니다. 파일 이름은 디렉토리 엔트리에 저장되고, 이 디렉토리 엔트리는 해당 파일의 I-Node 번호를 가지고 있습니다.

❓ window와 linux의 구조 차이에 대해 설명해주세요
</div>
</details>

<details>
<summary style="font-size:20px">동기와 비동기, 블로킹과 논블로킹</summary>
<div markdown="1">
<br/>
<h2>🤔 동기와 비동기, 블로킹과 논블로킹의 차이를 설명해 주세요.</h2>

<h3>📝 나의 대답</h3>

- 동기 처리는 하나의 작업이 요청한 다른 작업이 끝날 때까지를 기다린 후 이후 작업을 수행하는 것입니다.

- 비동기 처리는 다른 작업에 요청을 넘긴 후 기다리지 않고 또 다른 작업을 수행하는 것입니다.

- 블로킹은 하나의 작업이 수행될 때 CPU가 유휴상태에 존재해 대기하고 있는 것을 의미합니다.

- 논블로킹은 CPU가 유휴상태에 있지 않고 다른 작업을 처리하는 것을 의미합니다.

<h3>GPT의 대답</h3>

- 블로킹은 하나의 작업이 수행되는 동안, 그 작업이 완료될 때까지 다른 작업이 대기 상태에 머무는 것을 의미합니다. 이는 특히 I/O 작업에서 흔히 볼 수 있습니다. 반면, 논블로킹은 하나의 작업이 수행되는 동안, 그 작업이 완료되기를 기다리지 않고 다른 작업을 계속 수행하는 것을 의미합니다."

<h3>😀 블로킹을 프린터 작업, 논블로킹을 I/O Bound 작업이라고 생각합시다.</h3>

<br/>
<h2>🤔 동기이면서 논블로킹이고, 비동기이면서 블로킹인 경우는 의미가 있다고 할 수 있나요?</h2>

<h3>📝 나의 대답</h3>

- 아니요. 의미가 없다고 생각합니다. 보통 사람들이 사용할 땐, 동기-블로킹 그리고 비동기-논블로킹을 짝을 맞춰 사용합니다. 이유는 사용하는 컨셉이 비슷해 효율이 좋기 때문입니다.

- 예를 들어, 동기인데 논블로킹을 사용한다고 가정했을 때 I/O Bound 작업을 수행하고 응답이 오는 것까지 요청을 보낸 곳에서 기다려야합니다. 이는 리소스를 잘 활용하지 못하는 것이며 병목인 부분입니다.

<h3>GPT의 대답</h3>

- 동기/논블로킹 시나리오는 적절한 예가 "select"나 "poll" 같은 시스템 호출에서 볼 수 있습니다. 이들은 여러 I/O 작업을 동시에 모니터링하고, 어떤 작업이 먼저 완료되는지 알려주는 방식으로 동작합니다. 이 경우, 작업이 완료될 때까지 기다리지 않고 (논블로킹), 여러 작업을 동시에 체크하면서 (동기) 작업을 진행합니다.

- 비동기/블로킹의 시나리오는 어떤 작업이 실행 중일 때 다른 작업을 시작하고, 그 작업이 완료될 때까지 기다리는 것을 포함할 수 있습니다. 이런 경우는 드물지만, 예를 들어 특정 시점에서 작업의 완료를 기다리기 위해 블로킹을 선택할 수 있습니다.

<br/>
<h2>❗️ 외웁시다.</h2>

- 동기/비동기는 요청한 **작업에 대해 완료 여부**를 신경써서 작업을 순차적으로 수행할지 아닌지에 대한 관점입니다.

- 블로킹/논블로킹은 단어 그대로 현재 그대로 **작업이 block(차단, 대기) 되느냐 아니냐**에 따라 다른 작업을 수행할 수 있는지에 대한 관점입니다.

- 또한, 비동기는 출력 순서와 관련된 개념이고 논블로킹이 병렬 실행과 관련된 개념입니다.

<br/>
<h2>🤔 I/O 멀티플렉싱에 대해 설명해 주세요.</h2>

<h3>GPT의 대답</h3>

- I/O 멀티플렉싱(I/O Multiplexing)은 여러 I/O 작업을 동시에 처리할 수 있게 하는 기법을 말합니다. 여러 클라이언트와의 연결을 관리하거나 여러 I/O 작업을 동시에 처리해야 하는 네트워크 프로그래밍 등에서 주로 사용됩니다.

- I/O 멀티플렉싱을 사용하면 한 스레드나 프로세스에서 여러 클라이언트와 동시에 통신할 수 있게 됩니다. 이는 클라이언트와의 통신이 블로킹 연산을 필요로 하기 때문에 유용합니다. 즉, 데이터를 보내거나 받을 수 있을 때까지 기다리는 대신, 여러 I/O 작업들 중에서 준비된 작업을 선택하여 처리할 수 있습니다.

</div>
</details>